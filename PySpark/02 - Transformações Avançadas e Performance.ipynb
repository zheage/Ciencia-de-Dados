{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2080d3e-7a99-46b8-921f-0d5fa253b7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Particionamento e `repartition()` vs. `coalesce()` \n",
    "\n",
    "üîß O que √© particionamento no Spark?\n",
    "O particionamento define como os dados de um DataFrame s√£o distribu√≠dos fisicamente entre os n√≥s e n√∫cleos do cluster. Cada parti√ß√£o representa uma fatia de dados que pode ser processada paralelamente.\n",
    "\n",
    "‚úîÔ∏è Quanto melhor os dados estiverem particionados, maior o paralelismo, menor o custo de shuffle e melhor o uso de CPU e mem√≥ria.\n",
    "\n",
    "‚öôÔ∏è M√©todos de controle de particionamento\n",
    "## `repartition(numPartitions)`\n",
    "‚úÖ O que faz:\n",
    "- Cria novas parti√ß√µes redistribuindo completamente os dados via shuffle total.\n",
    "- Garante que as parti√ß√µes fiquem mais uniformes.\n",
    "\n",
    "üß† Quando usar:\n",
    "- Quando o DataFrame est√° desequilibrado (skewed).\n",
    "- Antes de um join pesado, para garantir distribui√ß√£o adequada.\n",
    "- Para aumentar o paralelismo de uma opera√ß√£o como write().\n",
    "\n",
    "## coalesce(numPartitions)\n",
    "‚úÖ O que faz:\n",
    "- Reduz o n√∫mero de parti√ß√µes sem embaralhar os dados (shuffle evitado).\n",
    "- Simplesmente funde parti√ß√µes j√° existentes.\n",
    "\n",
    "üß† Quando usar:\n",
    "- Antes de um salvamento para disco √∫nico (por exemplo, .toPandas(), .write.csv()).\n",
    "- Ao final do pipeline para evitar muitos arquivos pequenos no output.\n",
    "\n",
    "| Aspecto                  | `repartition()`                       | `coalesce()`                          |\n",
    "| ------------------------ | ------------------------------------- | ------------------------------------- |\n",
    "| Tipo de opera√ß√£o         | Com shuffle                           | Sem shuffle                           |\n",
    "| Pode aumentar parti√ß√µes? | ‚úÖ Sim                                 | ‚ùå N√£o (s√≥ reduz)                      |\n",
    "| Custo computacional      | Alto (embaralha dados)                | Baixo (mant√©m dados locais)           |\n",
    "| Ideal para               | Equalizar carga antes de joins/writes | Otimizar escrita com menos arquivos   |\n",
    "| Exemplo comum            | `df.repartition(200)`                 | `df.coalesce(1)` para salvar em 1 CSV |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f753f8f2-98c4-4f6d-b300-4daefb44aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, sequence\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Gerar uma lista de CPFs aleat√≥rios\n",
    "def gerar_cpf():\n",
    "    return ''.join([str(random.randint(0, 9)) for _ in range(11)])\n",
    "\n",
    "cpfs = [gerar_cpf() for _ in range(10000)]\n",
    "\n",
    "# Gerar uma lista de datas de refer√™ncia\n",
    "data_inicial = datetime(2025, 1, 1).date()\n",
    "datas_ref = [data_inicial + timedelta(days=i) for i in range(10000)]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(zip(cpfs, datas_ref), schema=[\"CPF\", \"DT_REF\"])\n",
    "\n",
    "# Reparticionar o DataFrame\n",
    "df_reparticionado = df.repartition(10)\n",
    "\n",
    "display(df_reparticionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589752ce-4911-49a6-9edc-ccfebbf4ac91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Broadcast joins e quando utiliz√°-los\n",
    "\n",
    "üìò O que √© um Broadcast Join?\n",
    "√â uma t√©cnica usada quando uma das tabelas do join √© significativamente menor que a outra.\n",
    "\n",
    "O Spark envia essa tabela pequena para todos os executores (n√≥s do cluster), permitindo que o join ocorra localmente em cada particionamento da tabela maior.\n",
    "\n",
    "‚öôÔ∏è Como funciona internamente\n",
    "Spark estima o tamanho das tabelas com base no metastore e estat√≠sticas de cardinalidade.\n",
    "\n",
    "Se detectar que uma das tabelas est√° abaixo de um limite configur√°vel (`spark.sql.autoBroadcastJoinThreshold`, padr√£o 10 MB), ele automaticamente aplica o broadcast join.\n",
    "\n",
    "> PySpark Broadcast Join is an important part of the SQL execution engine, With broadcast join, PySpark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that PySpark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor.\n",
    "\n",
    "Ou voc√™ pode for√ßar com `broadcast()` manualmente.\n",
    "\n",
    "| Vantagem                 | Explica√ß√£o                                                       |\n",
    "| ------------------------ | ---------------------------------------------------------------- |\n",
    "| üöÄ Performance           | Evita o **shuffle** da tabela maior ‚Üí menos I/O e tempo de rede. |\n",
    "| üíæ Efici√™ncia de mem√≥ria | √ötil quando a tabela pequena **cabe na mem√≥ria dos executores**. |\n",
    "| üîÅ Join Local            | Opera√ß√µes ocorrem localmente por parti√ß√£o, o que reduz lat√™ncia. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cdf768-6bc3-4462-aa5b-31929e307fb6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemplo da Utiliza√ß√£o do Broadcast"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# ================================\n",
    "# 1. Criar a tabela de contratos\n",
    "# ================================\n",
    "schema_contratos = StructType([\n",
    "    StructField(\"contrato_id\", IntegerType(), True),\n",
    "    StructField(\"cliente_id\", IntegerType(), True),\n",
    "    StructField(\"score_credito\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "dados_contratos = [\n",
    "    (101, 1, 745),\n",
    "    (102, 2, 620),\n",
    "    (103, 3, 590),\n",
    "    (104, 4, 710),\n",
    "    (105, 5, 800),\n",
    "    (106, 6, 670),\n",
    "]\n",
    "\n",
    "df_contratos = spark.createDataFrame(dados_contratos, schema=schema_contratos)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Criar a tabela de faixas de score (pequena)\n",
    "# ==========================================\n",
    "\n",
    "schema_faixas = StructType([\n",
    "    StructField(\"faixa_min\", IntegerType(), True),\n",
    "    StructField(\"faixa_max\", IntegerType(), True),\n",
    "    StructField(\"segmento\", StringType(), True)\n",
    "])\n",
    "\n",
    "dados_faixas = [\n",
    "    (300, 599, \"Alto Risco\"),\n",
    "    (600, 699, \"M√©dio Risco\"),\n",
    "    (700, 850, \"Baixo Risco\")\n",
    "]\n",
    "\n",
    "df_faixas_score = spark.createDataFrame(dados_faixas, schema=schema_faixas)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Realizar o broadcast join\n",
    "# ==========================================\n",
    "\n",
    "df_join = df_contratos.join(\n",
    "    broadcast(df_faixas_score),\n",
    "    (df_contratos[\"score_credito\"] >= df_faixas_score[\"faixa_min\"]) &\n",
    "    (df_contratos[\"score_credito\"] <= df_faixas_score[\"faixa_max\"]),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Exibir resultado\n",
    "# ==========================================\n",
    "display(df_join.select(\"contrato_id\", \"cliente_id\", \"score_credito\", \"segmento\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "085e0f57-45bf-4b16-98c5-7e6614f34c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Caching com `cache()` e `persist()`\n",
    "\n",
    "No Apache Spark, os m√©todos `cache()` e `persist()` s√£o utilizados para armazenar em mem√≥ria (ou em disco) os dados intermedi√°rios de um DataFrame ou RDD, com o objetivo de acelerar reprocessamentos subsequentes. Isso √© especialmente √∫til em pipelines de transforma√ß√£o que reutilizam os mesmos dados v√°rias vezes (ex.: treinamentos iterativos de modelos de machine learning ou ETL intensivo). A escolha entre eles depende de necessidades espec√≠ficas de performance e uso de mem√≥ria.\n",
    "\n",
    "##üîπ `cache()`: armazenamento padr√£o em mem√≥ria\n",
    "- **Defini√ß√£o:** `cache()` √© um atalho para `persist(StorageLevel.MEMORY_AND_DISK)` em PySpark, embora nos bastidores use por padr√£o `MEMORY_AND_DISK` ou `MEMORY_ONLY`, dependendo da vers√£o e linguagem.\n",
    "\n",
    "- **Comportamento:** Tenta armazenar os dados em mem√≥ria; se n√£o couberem completamente, Spark recomputa as parti√ß√µes n√£o armazenadas sempre que forem requisitadas.\n",
    "\n",
    "- **Uso t√≠pico:** Quando os dados cabem razoavelmente na mem√≥ria e o custo de recomputa√ß√£o n√£o √© cr√≠tico para as parti√ß√µes faltantes.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```\n",
    "df.cache()\n",
    "df.count()  # materializa o cache\n",
    "```\n",
    "\n",
    "Benef√≠cio real:\n",
    "\n",
    "```\n",
    "1¬™ chamada: df.count() -> 5.11 segundos\n",
    "2¬™ chamada: df.count() -> 0.44 segundos\n",
    "```\n",
    "\n",
    "##üîπ `persist(storageLevel):` controle granular de armazenamento\n",
    "- **Defini√ß√£o:** Permite escolher o n√≠vel de persist√™ncia desejado entre v√°rias op√ß√µes oferecidas pela enumera√ß√£o StorageLevel, como:\n",
    "\n",
    "- `MEMORY_ONLY`\n",
    "- `MEMORY_AND_DISK`\n",
    "- `DISK_ONLY`\n",
    "- `MEMORY_AND_DISK_SER` (vers√£o serializada)\n",
    "- `OFF_HEAP`\n",
    "\n",
    "- **Comportamento:** Armazena os dados no(s) n√≠vel(is) definidos. Se a mem√≥ria n√£o for suficiente e o n√≠vel permitir, armazena o excedente no disco.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "df.count()  # materializa a persist√™ncia\n",
    "```\n",
    "\n",
    "> Ganho similar ao cache(), com maior flexibilidade e controle sobre o comportamento em ambientes com press√£o de mem√≥ria\n",
    "\n",
    "##üîπ Quando utilizar?\n",
    "Use `cache()`:\n",
    "\n",
    "- Para opera√ß√µes leves e uso repetido dos dados durante uma √∫nica sess√£o;\n",
    "- Quando os dados cabem confortavelmente em mem√≥ria.\n",
    "\n",
    "Use `persist()`:\n",
    "\n",
    "- Quando os dados n√£o cabem totalmente em mem√≥ria;\n",
    "- Quando se deseja armazenamento serializado (reduz uso de heap) ou com fallback para disco;\n",
    "- Quando √© necess√°rio replicar os dados entre executores para toler√¢ncia a falhas (`MEMORY_ONLY_2`, etc.).\n",
    "\n",
    "##üîπ Boas pr√°ticas\n",
    "- **Materializa√ß√£o:** o cache s√≥ √© efetivado ap√≥s a execu√ß√£o de uma a√ß√£o que varre completamente o DataFrame, como `count()` ou `collect()`. A√ß√µes parciais como `take(1)` cacheiam apenas uma parti√ß√£oLearning Spark 2nd.\n",
    "\n",
    "- **Unpersist:** sempre remova dados da mem√≥ria ap√≥s o uso com `df.unpersist()` para liberar recursos."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Transforma√ß√µes Avan√ßadas e Performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
