{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2080d3e-7a99-46b8-921f-0d5fa253b7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Particionamento e `repartition()` vs. `coalesce()` \n",
    "\n",
    "🔧 O que é particionamento no Spark?\n",
    "O particionamento define como os dados de um DataFrame são distribuídos fisicamente entre os nós e núcleos do cluster. Cada partição representa uma fatia de dados que pode ser processada paralelamente.\n",
    "\n",
    "✔️ Quanto melhor os dados estiverem particionados, maior o paralelismo, menor o custo de shuffle e melhor o uso de CPU e memória.\n",
    "\n",
    "⚙️ Métodos de controle de particionamento\n",
    "## `repartition(numPartitions)`\n",
    "✅ O que faz:\n",
    "- Cria novas partições redistribuindo completamente os dados via shuffle total.\n",
    "- Garante que as partições fiquem mais uniformes.\n",
    "\n",
    "🧠 Quando usar:\n",
    "- Quando o DataFrame está desequilibrado (skewed).\n",
    "- Antes de um join pesado, para garantir distribuição adequada.\n",
    "- Para aumentar o paralelismo de uma operação como write().\n",
    "\n",
    "## coalesce(numPartitions)\n",
    "✅ O que faz:\n",
    "- Reduz o número de partições sem embaralhar os dados (shuffle evitado).\n",
    "- Simplesmente funde partições já existentes.\n",
    "\n",
    "🧠 Quando usar:\n",
    "- Antes de um salvamento para disco único (por exemplo, .toPandas(), .write.csv()).\n",
    "- Ao final do pipeline para evitar muitos arquivos pequenos no output.\n",
    "\n",
    "| Aspecto                  | `repartition()`                       | `coalesce()`                          |\n",
    "| ------------------------ | ------------------------------------- | ------------------------------------- |\n",
    "| Tipo de operação         | Com shuffle                           | Sem shuffle                           |\n",
    "| Pode aumentar partições? | ✅ Sim                                 | ❌ Não (só reduz)                      |\n",
    "| Custo computacional      | Alto (embaralha dados)                | Baixo (mantém dados locais)           |\n",
    "| Ideal para               | Equalizar carga antes de joins/writes | Otimizar escrita com menos arquivos   |\n",
    "| Exemplo comum            | `df.repartition(200)`                 | `df.coalesce(1)` para salvar em 1 CSV |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f753f8f2-98c4-4f6d-b300-4daefb44aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, sequence\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Gerar uma lista de CPFs aleatórios\n",
    "def gerar_cpf():\n",
    "    return ''.join([str(random.randint(0, 9)) for _ in range(11)])\n",
    "\n",
    "cpfs = [gerar_cpf() for _ in range(10000)]\n",
    "\n",
    "# Gerar uma lista de datas de referência\n",
    "data_inicial = datetime(2025, 1, 1).date()\n",
    "datas_ref = [data_inicial + timedelta(days=i) for i in range(10000)]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(zip(cpfs, datas_ref), schema=[\"CPF\", \"DT_REF\"])\n",
    "\n",
    "# Reparticionar o DataFrame\n",
    "df_reparticionado = df.repartition(10)\n",
    "\n",
    "display(df_reparticionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589752ce-4911-49a6-9edc-ccfebbf4ac91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Broadcast joins e quando utilizá-los\n",
    "\n",
    "📘 O que é um Broadcast Join?\n",
    "É uma técnica usada quando uma das tabelas do join é significativamente menor que a outra.\n",
    "\n",
    "O Spark envia essa tabela pequena para todos os executores (nós do cluster), permitindo que o join ocorra localmente em cada particionamento da tabela maior.\n",
    "\n",
    "⚙️ Como funciona internamente\n",
    "Spark estima o tamanho das tabelas com base no metastore e estatísticas de cardinalidade.\n",
    "\n",
    "Se detectar que uma das tabelas está abaixo de um limite configurável (`spark.sql.autoBroadcastJoinThreshold`, padrão 10 MB), ele automaticamente aplica o broadcast join.\n",
    "\n",
    "> PySpark Broadcast Join is an important part of the SQL execution engine, With broadcast join, PySpark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that PySpark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor.\n",
    "\n",
    "Ou você pode forçar com `broadcast()` manualmente.\n",
    "\n",
    "| Vantagem                 | Explicação                                                       |\n",
    "| ------------------------ | ---------------------------------------------------------------- |\n",
    "| 🚀 Performance           | Evita o **shuffle** da tabela maior → menos I/O e tempo de rede. |\n",
    "| 💾 Eficiência de memória | Útil quando a tabela pequena **cabe na memória dos executores**. |\n",
    "| 🔁 Join Local            | Operações ocorrem localmente por partição, o que reduz latência. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cdf768-6bc3-4462-aa5b-31929e307fb6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemplo da Utilização do Broadcast"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# ================================\n",
    "# 1. Criar a tabela de contratos\n",
    "# ================================\n",
    "schema_contratos = StructType([\n",
    "    StructField(\"contrato_id\", IntegerType(), True),\n",
    "    StructField(\"cliente_id\", IntegerType(), True),\n",
    "    StructField(\"score_credito\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "dados_contratos = [\n",
    "    (101, 1, 745),\n",
    "    (102, 2, 620),\n",
    "    (103, 3, 590),\n",
    "    (104, 4, 710),\n",
    "    (105, 5, 800),\n",
    "    (106, 6, 670),\n",
    "]\n",
    "\n",
    "df_contratos = spark.createDataFrame(dados_contratos, schema=schema_contratos)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Criar a tabela de faixas de score (pequena)\n",
    "# ==========================================\n",
    "\n",
    "schema_faixas = StructType([\n",
    "    StructField(\"faixa_min\", IntegerType(), True),\n",
    "    StructField(\"faixa_max\", IntegerType(), True),\n",
    "    StructField(\"segmento\", StringType(), True)\n",
    "])\n",
    "\n",
    "dados_faixas = [\n",
    "    (300, 599, \"Alto Risco\"),\n",
    "    (600, 699, \"Médio Risco\"),\n",
    "    (700, 850, \"Baixo Risco\")\n",
    "]\n",
    "\n",
    "df_faixas_score = spark.createDataFrame(dados_faixas, schema=schema_faixas)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Realizar o broadcast join\n",
    "# ==========================================\n",
    "\n",
    "df_join = df_contratos.join(\n",
    "    broadcast(df_faixas_score),\n",
    "    (df_contratos[\"score_credito\"] >= df_faixas_score[\"faixa_min\"]) &\n",
    "    (df_contratos[\"score_credito\"] <= df_faixas_score[\"faixa_max\"]),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Exibir resultado\n",
    "# ==========================================\n",
    "display(df_join.select(\"contrato_id\", \"cliente_id\", \"score_credito\", \"segmento\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "085e0f57-45bf-4b16-98c5-7e6614f34c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Caching com `cache()` e `persist()`\n",
    "\n",
    "No Apache Spark, os métodos `cache()` e `persist()` são utilizados para armazenar em memória (ou em disco) os dados intermediários de um DataFrame ou RDD, com o objetivo de acelerar reprocessamentos subsequentes. Isso é especialmente útil em pipelines de transformação que reutilizam os mesmos dados várias vezes (ex.: treinamentos iterativos de modelos de machine learning ou ETL intensivo). A escolha entre eles depende de necessidades específicas de performance e uso de memória.\n",
    "\n",
    "##🔹 `cache()`: armazenamento padrão em memória\n",
    "- **Definição:** `cache()` é um atalho para `persist(StorageLevel.MEMORY_AND_DISK)` em PySpark, embora nos bastidores use por padrão `MEMORY_AND_DISK` ou `MEMORY_ONLY`, dependendo da versão e linguagem.\n",
    "\n",
    "- **Comportamento:** Tenta armazenar os dados em memória; se não couberem completamente, Spark recomputa as partições não armazenadas sempre que forem requisitadas.\n",
    "\n",
    "- **Uso típico:** Quando os dados cabem razoavelmente na memória e o custo de recomputação não é crítico para as partições faltantes.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```\n",
    "df.cache()\n",
    "df.count()  # materializa o cache\n",
    "```\n",
    "\n",
    "Benefício real:\n",
    "\n",
    "```\n",
    "1ª chamada: df.count() -> 5.11 segundos\n",
    "2ª chamada: df.count() -> 0.44 segundos\n",
    "```\n",
    "\n",
    "##🔹 `persist(storageLevel):` controle granular de armazenamento\n",
    "- **Definição:** Permite escolher o nível de persistência desejado entre várias opções oferecidas pela enumeração StorageLevel, como:\n",
    "\n",
    "- `MEMORY_ONLY`\n",
    "- `MEMORY_AND_DISK`\n",
    "- `DISK_ONLY`\n",
    "- `MEMORY_AND_DISK_SER` (versão serializada)\n",
    "- `OFF_HEAP`\n",
    "\n",
    "- **Comportamento:** Armazena os dados no(s) nível(is) definidos. Se a memória não for suficiente e o nível permitir, armazena o excedente no disco.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "df.count()  # materializa a persistência\n",
    "```\n",
    "\n",
    "> Ganho similar ao cache(), com maior flexibilidade e controle sobre o comportamento em ambientes com pressão de memória\n",
    "\n",
    "##🔹 Quando utilizar?\n",
    "Use `cache()`:\n",
    "\n",
    "- Para operações leves e uso repetido dos dados durante uma única sessão;\n",
    "- Quando os dados cabem confortavelmente em memória.\n",
    "\n",
    "Use `persist()`:\n",
    "\n",
    "- Quando os dados não cabem totalmente em memória;\n",
    "- Quando se deseja armazenamento serializado (reduz uso de heap) ou com fallback para disco;\n",
    "- Quando é necessário replicar os dados entre executores para tolerância a falhas (`MEMORY_ONLY_2`, etc.).\n",
    "\n",
    "##🔹 Boas práticas\n",
    "- **Materialização:** o cache só é efetivado após a execução de uma ação que varre completamente o DataFrame, como `count()` ou `collect()`. Ações parciais como `take(1)` cacheiam apenas uma partiçãoLearning Spark 2nd.\n",
    "\n",
    "- **Unpersist:** sempre remova dados da memória após o uso com `df.unpersist()` para liberar recursos."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Transformações Avançadas e Performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
