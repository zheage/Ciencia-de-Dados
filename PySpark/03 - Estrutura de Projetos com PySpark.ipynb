{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b54c3a6-35fb-47c5-8da9-e68bb5167069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Organização de scripts em pipelines  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93fd0f8f-4c04-4a76-bc44-6621ac36ea3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Arquitetura Geral de Diretórios"
    }
   },
   "source": [
    "## Arquitetura Geral de Diretórios\n",
    "\n",
    "```\n",
    "├── config/\n",
    "│   └── spark_config.py          # Inicialização da SparkSession\n",
    "├── src/\n",
    "│   ├── extract/\n",
    "│   │   └── load_data.py         # Lê dados de fontes (Parquet, Hive, etc.)\n",
    "│   ├── transform/\n",
    "│   │   ├── feature_engineering.py\n",
    "│   │   └── aggregations.py\n",
    "│   ├── validate/\n",
    "│   │   └── quality_checks.py    # Regras de qualidade, schemas\n",
    "│   ├── model/\n",
    "│   │   └── train_model.py       # (se for um pipeline de ML)\n",
    "│   └── utils/\n",
    "│       └── helpers.py           # Funções auxiliares\n",
    "├── jobs/\n",
    "│   └── run_pipeline.py          # Pipeline orquestrado\n",
    "├── tests/\n",
    "│   └── test_transform.py\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "## Modularize por fase\n",
    "\n",
    "- Cada etapa da pipeline deve ser uma função bem definida, e cada módulo faz apenas uma coisa\n",
    "\n",
    "```python\n",
    "def add_flags(df):\n",
    "    return df.withColumn(\"flag_novo\", F.col(\"dias_atraso\") > 90)\n",
    "\n",
    "def enrich_with_cep(df, df_cep):\n",
    "    return df.join(broadcast(df_cep), on=\"cep\", how=\"left\")\n",
    "```\n",
    "\n",
    "## Script principal (jobs/run_pipeline.py)\n",
    "\n",
    "```python\n",
    "from config.spark_config import get_spark\n",
    "from src.extract.load_data import load_clientes, load_pagamentos\n",
    "from src.transform.feature_engineering import add_flags\n",
    "from src.validate.quality_checks import assert_no_nulls\n",
    "\n",
    "spark = get_spark()\n",
    "\n",
    "df_cli = load_clientes(spark)\n",
    "df_pag = load_pagamentos(spark)\n",
    "\n",
    "df_merged = df_cli.join(df_pag, \"id_cliente\")\n",
    "df_final = add_flags(df_merged)\n",
    "\n",
    "assert_no_nulls(df_final, \"flag_novo\")\n",
    "\n",
    "df_final.write.mode(\"overwrite\").parquet(\".../output/df_final.parquet\")\n",
    "``` \n",
    "\n",
    "## Evite Anti-padrões\n",
    "\n",
    "- Não coloque tudo em um único notebook\n",
    "- Não acople leitura, transformação e escrita na mesma função\n",
    "- Não misture lógica de negócios com parsing de argumentos ou paths\n",
    "\n",
    "## Inclua testes simples\n",
    "\n",
    "Com pytest, teste sua lógica local com DataFrames pequenos (via spark.createDataFrame(...)).\n",
    "Isso é essencial para validação de features em modelos de risco regulatórios."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Estrutura de Projetos com PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
