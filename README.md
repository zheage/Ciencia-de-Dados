# üß† Reposit√≥rio de Anota√ß√µes em Ci√™ncia de Dados

Este reposit√≥rio re√∫ne minhas anota√ß√µes pessoais, resumos t√©cnicos e aprendizados pr√°ticos ao longo da minha jornada como Cientista de Dados.

O objetivo √© consolidar, revisar e compartilhar conhecimentos de forma organizada, cobrindo temas essenciais da √°rea ‚Äî desde fundamentos estat√≠sticos at√© aplica√ß√µes avan√ßadas em machine learning, engenharia de dados e regulamenta√ß√µes aplicadas ao setor financeiro (como IFRS 9 e Resolu√ß√£o 4.966 do BACEN).

---

### üîñ Tipos de commit (padroniza√ß√£o)

| Tag        | Descri√ß√£o                                             |
|------------|--------------------------------------------------------|
| `docs`     | Altera√ß√£o de documenta√ß√£o                              |
| `experiment` | Testes explorat√≥rios                                 |
| `feature`  | Nova funcionalidade                                    |
| `fix`      | Corre√ß√£o de erro                                       |
| `model`    | Treinamento ou tuning de modelo                        |
| `perf`     | Mudan√ßa de c√≥digo focada em melhorar performance       |
| `refactor` | Refatora√ß√£o de c√≥digo existente (sem mudar funcionalidade) |

---

# üìò Trilha de Estudos: Machine Learning para Risco de Cr√©dito (com base no ESL)

> Baseado no livro _The Elements of Statistical Learning_ com aplica√ß√µes em inadimpl√™ncia, scorecards e IFRS 9.

---

## üß± M√≥dulo 1 ‚Äì Fundamentos Estat√≠sticos e Pr√©-processamento

- [ ] Refor√ßar conceitos de probabilidade e infer√™ncia com Casella & Berger
- [ ] Realizar an√°lise explorat√≥ria (EDA) em bases como HMEQ
- [ ] Identificar e tratar outliers e valores ausentes
- [ ] Aplicar t√©cnicas de normaliza√ß√£o, binning supervisionado e WOE

---

## üå≤ M√≥dulo 2 ‚Äì M√©todos Lineares para Classifica√ß√£o

- [ ] Estudar Regress√£o Log√≠stica (Cap. 4 do ESL)
- [ ] Explorar regulariza√ß√£o Lasso e Ridge (Cap. 3)
- [ ] Aplicar An√°lise Discriminante Linear (LDA)
- [ ] Implementar um scorecard com `scorecardpy` ou `statsmodels`

---

## üå≥ M√≥dulo 3 ‚Äì M√©todos N√£o Lineares e √Årvores

- [ ] Entender √°rvores de decis√£o e suas divis√µes (Cap. 9)
- [ ] Aplicar Random Forest e Bagging (Cap. 15)
- [ ] Implementar modelos com Boosting (Cap. 10) usando LightGBM
- [ ] Comparar performance entre Logit e Gradient Boosting com HMEQ

---

## ‚õì M√≥dulo 4 ‚Äì Modelos de Tempo e Sobreviv√™ncia

- [ ] Estudar Hazard Models (discreto e cont√≠nuo)
- [ ] Aplicar o modelo de Cox com `lifelines`
- [ ] Trabalhar com covari√°veis dependentes do tempo
- [ ] Implementar modelagem de PD com sobreviv√™ncia (ex: `pycox`)

---

## üß™ M√≥dulo 5 ‚Äì Valida√ß√£o, Performance e Dados Desbalanceados

- [ ] Avaliar modelos com AUC, KS, Lift, Precision/Recall
- [ ] Tratar dados desbalanceados com SMOTE e undersampling
- [ ] Calcular e monitorar o PSI para estabilidade de vari√°veis
- [ ] Implementar valida√ß√£o cruzada e holdout temporal

---

## üß† M√≥dulo 6 ‚Äì Modelos Avan√ßados e Interpreta√ß√£o

- [ ] Explorar SVM e Kernel Methods (Cap. 12)
- [ ] Estudar t√©cnicas de explicabilidade (SHAP, LIME)
- [ ] Trabalhar com modelos de m√∫ltiplas sa√≠das (Cap. 11)
- [ ] Estimar PD, LGD e EAD em tarefas multi-target

---

## üì¶ Recursos Complementares

- [ ] Baixar bases p√∫blicas: HMEQ, Lending Club, Home Credit (Kaggle)
- [ ] Estudar IFRS 9, Resolu√ß√£o CMN 4966 e Basel II/III
- [ ] Familiarizar-se com `scikit-learn`, `lightgbm`, `lifelines`, `statsmodels`
- [ ] Revisar cap√≠tulos espec√≠ficos no livro _Credit Risk Analytics_ (Baesens)

---

# ‚ö° Trilha de Estudos PySpark

Checklist com os principais t√≥picos que um cientista de dados deve dominar para trabalhar com PySpark em ambientes de big data, incluindo transforma√ß√µes, modelagem e performance.

## ‚úÖ Fundamentos do PySpark
- [ ] O que √© PySpark e como ele se integra com Apache Spark  
- [ ] Instala√ß√£o e primeiros passos (SparkSession, SparkContext)  
- [ ] Diferen√ßas entre RDD, DataFrame e Dataset  
- [ ] Leitura e escrita de arquivos (CSV, JSON, Parquet)  
- [ ] Schema expl√≠cito vs. infer√™ncia autom√°tica  
- [ ] Trabalhando com `show()`, `printSchema()`, `describe()`, `select()`  

## üß™ Manipula√ß√£o de Dados com DataFrames
- [ ] Filtros (`filter()`, `where()`)  
- [ ] Sele√ß√£o e renomea√ß√£o de colunas  
- [ ] Cria√ß√£o de colunas com `withColumn()` e `expr()`  
- [ ] Fun√ß√µes nativas (`when`, `col`, `lit`, `isNull`, `isin`)  
- [ ] Joins (`inner`, `left`, `right`, `outer`, `semi`, `anti`)  
- [ ] Agrupamentos e agrega√ß√µes com `groupBy()`  
- [ ] Window functions (`row_number`, `rank`, `lag`, `lead`)  
- [ ] Tratamento de valores nulos e duplicados  

## üîÑ Transforma√ß√µes Avan√ßadas e Performance
- [ ] Lazy evaluation: como funciona e por que importa  
- [ ] Particionamento e `repartition()` vs. `coalesce()`  
- [ ] Broadcast joins e quando utiliz√°-los  
- [ ] Caching com `cache()` e `persist()`  
- [ ] UDFs e Pandas UDFs (User Defined Functions)  
- [ ] Leitura eficiente com `.option()`, `.schema()`, `.mode()`  

## üß± Estrutura de Projetos com PySpark
- [ ] Organiza√ß√£o de scripts em pipelines  
- [ ] Modulariza√ß√£o de jobs com fun√ß√µes e classes  
- [ ] Par√¢metros via `argparse` ou arquivos `.yaml/.json`  
- [ ] Integra√ß√£o com Git e versionamento de c√≥digo Spark  

## üîÅ PySpark SQL
- [ ] Uso de SQL diretamente com `spark.sql()`  
- [ ] Registro de views tempor√°rias e permanentes  
- [ ] Convers√£o entre SQL e API DataFrame  

## üìà Machine Learning com MLlib
- [ ] Conceito de pipelines (`Pipeline`, `PipelineModel`)  
- [ ] Estimators e Transformers  
- [ ] Feature engineering com `VectorAssembler`, `StringIndexer`, `OneHotEncoder`  
- [ ] Treinamento de modelos (`LogisticRegression`, `RandomForest`, etc.)  
- [ ] Avalia√ß√£o com `BinaryClassificationEvaluator`, `RegressionEvaluator`  
- [ ] Cross-validation com `ParamGridBuilder` e `CrossValidator`  

## üåê Integra√ß√£o com o ecossistema
- [ ] Integra√ß√£o com Hive e Cat√°logo externo  
- [ ] Conex√£o com bancos de dados via JDBC  
- [ ] Escrita em Delta Lake (se aplic√°vel)  
- [ ] Execu√ß√£o no Databricks ou EMR  
- [ ] Integra√ß√£o com ferramentas de orquestra√ß√£o (Airflow, dbutils, etc.)

## üìö Extras e Boas Pr√°ticas
- [ ] Logging com `log4j` e controle de erros  
- [ ] Testes unit√°rios com `pytest` em c√≥digo PySpark  
- [ ] Uso de `config` e tuning com `spark.conf.set()`  
- [ ] Gerenciamento de recursos: executores, mem√≥ria, parti√ß√µes  
- [ ] Debugging em cluster (logs e UI do Spark)  
- [ ] An√°lise de DAGs no Spark UI  

---

# üß† Trilha de Programa√ß√£o Orientada a Objetos

Uma lista com os principais t√≥picos de POO que um cientista de dados deve dominar para estruturar c√≥digos mais robustos, reutiliz√°veis e escal√°veis com Python.

## ‚úÖ Fundamentos da Programa√ß√£o Orientada a Objetos
- [ ] O que √© POO? Paradigmas imperativos vs. orientados a objetos  
- [ ] Conceitos de Classe e Objeto  
- [ ] Atributos de inst√¢ncia vs. atributos de classe  
- [ ] M√©todos de inst√¢ncia  
- [ ] Construtor (`__init__`) e inicializa√ß√£o de objetos  
- [ ] Representa√ß√£o com `__str__()` e `__repr__()`  
- [ ] Atributos privados (`_`, `__`) e conven√ß√µes

## üîÅ Encapsulamento e Propriedades
- [ ] Encapsulamento: o que √© e por que importa  
- [ ] Getters e setters em Python  
- [ ] Uso do decorador `@property`  
- [ ] Controle de acesso (simulado) com underscores

## üß¨ Heran√ßa e Composi√ß√£o
- [ ] Heran√ßa simples e m√∫ltipla  
- [ ] `super()` e chamada da superclasse  
- [ ] Override de m√©todos  
- [ ] `isinstance()` e `issubclass()`  
- [ ] Composi√ß√£o vs. heran√ßa (prefer√™ncia por composi√ß√£o)

## üß© Polimorfismo
- [ ] M√©todos com o mesmo nome em classes diferentes  
- [ ] Duck typing: ‚Äúif it quacks like a duck‚Ä¶‚Äù  
- [ ] Uso pr√°tico em c√≥digo gen√©rico e testes

## üß± Classes Abstratas e Interfaces
- [ ] M√≥dulo `abc` e classe `ABC`  
- [ ] M√©todos abstratos com `@abstractmethod`  
- [ ] Por que usar classes abstratas em pipelines de ML ou ETL?

## üì¶ Organiza√ß√£o e Design
- [ ] M√≥dulos e pacotes em Python  
- [ ] Organiza√ß√£o de m√∫ltiplas classes em um projeto  
- [ ] Invers√£o de depend√™ncia b√°sica  
- [ ] SOLID principles (resumidamente)

## üß™ Aplica√ß√µes Pr√°ticas em Ci√™ncia de Dados
- [ ] Criar uma classe `Dataset` que encapsula limpeza, valida√ß√£o e transforma√ß√£o  
- [ ] Criar uma classe `FeatureEngineer` com m√©todos como `.scale()`, `.encode()`  
- [ ] Classe `ModelWrapper` para encapsular modelo, predict e m√©tricas  
- [ ] Abstra√ß√£o para m√∫ltiplos modelos (`Regressor`, `Classifier`, etc.)  
- [ ] Implementa√ß√£o de pipelines com objetos customizados  
- [ ] Projeto final: mini framework com suas pr√≥prias classes de modelagem

## üß† Extras (bons diferenciais)
- [ ] Uso de `__slots__` para otimiza√ß√£o de mem√≥ria  
- [ ] M√©todos m√°gicos (`__eq__`, `__len__`, `__iter__`, etc.)  
- [ ] Introdu√ß√£o a design patterns (Factory, Strategy, etc.)  
- [ ] Decoradores de classe e `@classmethod`, `@staticmethod`  
- [ ] Testes com classes: usando `pytest` e `unittest`
