# üß† Reposit√≥rio de Anota√ß√µes em Ci√™ncia de Dados

Este reposit√≥rio re√∫ne minhas anota√ß√µes pessoais, resumos t√©cnicos e aprendizados pr√°ticos ao longo da minha jornada como Cientista de Dados.

O objetivo √© consolidar, revisar e compartilhar conhecimentos de forma organizada, cobrindo temas essenciais da √°rea ‚Äî desde fundamentos estat√≠sticos at√© aplica√ß√µes avan√ßadas em machine learning, engenharia de dados e regulamenta√ß√µes aplicadas ao setor financeiro (como IFRS 9 e Resolu√ß√£o 4.966 do BACEN).

---

### üîñ Tipos de commit (padroniza√ß√£o)

| Tag        | Descri√ß√£o                                              |
|------------|--------------------------------------------------------|
| `docs`     | Altera√ß√£o de documenta√ß√£o                              |
| `experiment` | Testes explorat√≥rios                                 |
| `feature`  | Nova funcionalidade                                    |
| `fix`      | Corre√ß√£o de erro                                       |
| `model`    | Treinamento ou tuning de modelo                        |
| `perf`     | Mudan√ßa de c√≥digo focada em melhorar performance       |
| `refactor` | Refatora√ß√£o de c√≥digo existente (sem mudar funcionalidade) |

---

# ‚ö° Trilha de Estudos PySpark

Checklist com os principais t√≥picos que um cientista de dados deve dominar para trabalhar com PySpark em ambientes de big data, incluindo transforma√ß√µes, modelagem e performance.

## ‚úÖ Fundamentos do PySpark
- [ ] O que √© PySpark e como ele se integra com Apache Spark  
- [ ] Instala√ß√£o e primeiros passos (SparkSession, SparkContext)  
- [ ] Diferen√ßas entre RDD, DataFrame e Dataset  
- [ ] Leitura e escrita de arquivos (CSV, JSON, Parquet)  
- [ ] Schema expl√≠cito vs. infer√™ncia autom√°tica  
- [x] Trabalhando com `show()`, `printSchema()`, `describe()`, `select()`  
- [ ] Trabalhando com `show()`, `printSchema()`, `describe()`, `select()`  n

## üß™ Manipula√ß√£o de Dados com DataFrames
- [ ] Filtros (`filter()`, `where()`)  
- [ ] Sele√ß√£o e renomea√ß√£o de colunas  
- [ ] Cria√ß√£o de colunas com `withColumn()` e `expr()`  
- [ ] Fun√ß√µes nativas (`when`, `col`, `lit`, `isNull`, `isin`)  
- [ ] Joins (`inner`, `left`, `right`, `outer`, `semi`, `anti`)  
- [ ] Agrupamentos e agrega√ß√µes com `groupBy()`  
- [ ] Window functions (`row_number`, `rank`, `lag`, `lead`)  
- [ ] Tratamento de valores nulos e duplicados  

## üîÑ Transforma√ß√µes Avan√ßadas e Performance
- [ ] Lazy evaluation: como funciona e por que importa  
- [x] Particionamento e `repartition()` vs. `coalesce()`  
- [x] Particionamento: `repartition()` vs. `coalesce()`  
- [x] Broadcast joins e quando utiliz√°-los  
- [x] Caching com `cache()` e `persist()`  
- [ ] UDFs e Pandas UDFs (User Defined Functions)  
- [ ] Leitura eficiente com `.option()`, `.schema()`, `.mode()`  

## üß± Estrutura de Projetos com PySpark
- [x] Organiza√ß√£o de scripts em pipelines  
- [ ] Modulariza√ß√£o de jobs com fun√ß√µes e classes  
- [ ] Par√¢metros via `argparse` ou arquivos `.yaml/.json`  
- [ ] Integra√ß√£o com Git e versionamento de c√≥digo Spark  

## üîÅ PySpark SQL
- [x] Uso de SQL diretamente com `spark.sql()`  
- [x] Registro de views tempor√°rias e permanentes  
- [ ] Convers√£o entre SQL e API DataFrame  

## üìà Machine Learning com MLlib
- [ ] Conceito de pipelines (`Pipeline`, `PipelineModel`)  
- [ ] Estimators e Transformers  
- [ ] Feature engineering com `VectorAssembler`, `StringIndexer`, `OneHotEncoder`  
- [ ] Treinamento de modelos (`LogisticRegression`, `RandomForest`, etc.)  
- [ ] Avalia√ß√£o com `BinaryClassificationEvaluator`, `RegressionEvaluator`  
- [ ] Cross-validation com `ParamGridBuilder` e `CrossValidator`  

## üåê Integra√ß√£o com o ecossistema
- [ ] Integra√ß√£o com Hive e Cat√°logo externo  
- [ ] Conex√£o com bancos de dados via JDBC  
- [ ] Escrita em Delta Lake (se aplic√°vel)  
- [ ] Execu√ß√£o no Databricks ou EMR  
- [ ] Integra√ß√£o com ferramentas de orquestra√ß√£o (Airflow, dbutils, etc.)

## üìö Extras e Boas Pr√°ticas
- [ ] Logging com `log4j` e controle de erros  
- [ ] Testes unit√°rios com `pytest` em c√≥digo PySpark  
- [ ] Uso de `config` e tuning com `spark.conf.set()`  
- [ ] Gerenciamento de recursos: executores, mem√≥ria, parti√ß√µes  
- [ ] Debugging em cluster (logs e UI do Spark)  
- [ ] An√°lise de DAGs no Spark UI  

---

## ‚úÖ Roadmap SQL


## üìò Fundamentos de SQL
- [x] **SELECT, FROM, WHERE** ‚Äì Sintaxe b√°sica para consultar dados de uma tabela.
- [ ] **Operadores (IN, LIKE, BETWEEN, IS NULL)** ‚Äì Filtros comuns para valores, padr√µes e faixas.
- [x] **ORDER BY, LIMIT** ‚Äì Ordena√ß√£o de resultados e limita√ß√£o de registros retornados.
- [x] **Aliases (`AS`)** ‚Äì Dar nomes alternativos a colunas ou tabelas para facilitar a leitura.

## üßÆ Agrega√ß√µes e Agrupamentos
- [x] **COUNT, SUM, AVG, MIN, MAX** ‚Äì Fun√ß√µes agregadoras para sumarizar dados.
- [x] **GROUP BY** ‚Äì Agrupar registros por uma ou mais colunas.
- [ ] **HAVING** ‚Äì Filtrar resultados ap√≥s o `GROUP BY`.
- [x] **DISTINCT** ‚Äì Remover duplicatas nos resultados.

## üîó Joins (Jun√ß√µes entre tabelas)
- [x] **INNER JOIN** ‚Äì Retorna registros com correspond√™ncia nas duas tabelas.
- [x] **LEFT JOIN / RIGHT JOIN** ‚Äì Inclui todos os registros da tabela da esquerda/direita.
- [ ] **FULL OUTER JOIN** ‚Äì Inclui todos os registros de ambas as tabelas, com ou sem correspond√™ncia.
- [ ] **CROSS JOIN** ‚Äì Produto cartesiano entre duas tabelas.
- [ ] **SELF JOIN** ‚Äì Tabela unida com ela mesma para rela√ß√µes hier√°rquicas ou compara√ß√µes.

## üß± Subqueries e CTEs
- [ ] **Subqueries em SELECT, FROM, WHERE** ‚Äì Consultas aninhadas para filtragem ou c√°lculo.
- [ ] **CTE com `WITH`** ‚Äì Nomear subqueries reutiliz√°veis, melhorando legibilidade.
- [ ] **CTE Recursiva** ‚Äì Para hierarquias (ex: organogramas, categorias encadeadas).

## ü™ü Window Functions (Fun√ß√µes de Janela)
- [ ] **ROW_NUMBER, RANK, DENSE_RANK** ‚Äì Ordenar e numerar registros dentro de parti√ß√µes.
- [ ] **LAG, LEAD** ‚Äì Acessar valores anteriores ou posteriores sem auto-joins.
- [ ] **FIRST_VALUE, LAST_VALUE** ‚Äì Retornar o primeiro ou √∫ltimo valor da parti√ß√£o.
- [ ] **OVER (PARTITION BY ... ORDER BY ...)** ‚Äì Definir o escopo e ordem das janelas.

## üßπ Manipula√ß√£o e Limpeza de Dados
- [ ] **CASE WHEN** ‚Äì Express√µes condicionais tipo `if/else` para criar colunas derivadas.
- [ ] **COALESCE, NULLIF** ‚Äì Substitui√ß√£o e compara√ß√£o com valores nulos.
- [ ] **CAST, CONVERT** ‚Äì Convers√£o entre tipos (string, inteiro, data etc.).
- [ ] **TRIM, SUBSTRING, UPPER, LOWER** ‚Äì Fun√ß√µes de texto e limpeza de strings.

## üóÇÔ∏è Modelagem e Performance
- [ ] **Normaliza√ß√£o e Desnormaliza√ß√£o** ‚Äì Estruturar dados para consist√™ncia e/ou performance.
- [ ] **CREATE TABLE, tipos de dados** ‚Äì Definir esquemas e estruturas de tabelas.
- [ ] **√çndices e Chaves** ‚Äì Melhorar performance de busca e integridade referencial.
- [ ] **EXPLAIN PLAN / ANALYZE** ‚Äì Diagnosticar e otimizar performance de queries.

## üß† SQL Avan√ßado
- [ ] **PIVOT / UNPIVOT** ‚Äì Transformar linhas em colunas e vice-versa.
- [ ] **UDFs (User Defined Functions)** ‚Äì Criar fun√ß√µes customizadas em SQL ou integradas com Python.
- [ ] **Views e Materialized Views** ‚Äì Criar camadas reutiliz√°veis de consulta.
- [ ] **Tabelas Tempor√°rias e Persistentes** ‚Äì Gerenciar escopo e dura√ß√£o de dados intermedi√°rios.

---

